alg_name: UNKE-ARE
model_name: gpt2-xl
device: 0

# Method Hyperparameters
layers: [10, 11, 12, 13, 14]
layer_selection: all
fact_token: last
lr: 5e-4
v_num_grad_steps: 50
v_lr: 5e-1
v_loss_layer: 47
v_weight_decay: 0.5
clamp_norm_factor: 4
optim_num_step: 20
ex_data_num: 3
window_size: 10
overlap: 3

# Module templates
rewrite_module_tmp: "transformer.h.{}.mlp"
layer_module_tmp: "transformer.h.{}"
mlp_module_tmp: "transformer.h.{}.mlp"
attn_module_tmp: "transformer.h.{}.attn"
ln_f_module: "transformer.ln_f"
lm_head_module: "transformer.wte"

# Model parameters
batch_size: 1
max_length: 40
model_parallel: false 