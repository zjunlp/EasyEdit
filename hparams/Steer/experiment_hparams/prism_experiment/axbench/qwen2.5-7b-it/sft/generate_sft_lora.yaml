# === Basic Config ===
alg_name: sft
layers: [14]
use_cache: false

# === Dataset Config ===
exclude_bos: true
max_concepts: 1
max_num_of_examples: null
preference_pairs: ["orig_add", "orig_sub"] # use_cmd_to_define
output_length: 512

# === Training Config ===
batch_size: 2 # the actual batch size also needs to multiply with |preference_pairs|
gradient_accumulation_steps: 6
n_epochs: 12
lr: 0.08
dropout: 0.1
lora_alpha: 32

weight_decay: 0.00
pos_loss_weight: 1.0     
neg_loss_weight: 0.0    
margin_penalty_weight: 0  
ref_loss_weight: 0.0      
margin_threshold: 0.0  

loss_output_dir: null
inference: false

# === Intervention Config ===
intervention_components: "mlp_mid"
intervention_positions: "all"
intervention_positions_dropout: 0.0
intervention_type: "addition" # clamping
intervention_method: "lora"
low_rank_dimension: 4

# === Steering Config ===
steering_factors: [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0] # use_cmd_to_define

steering_prompt_type: "blend_in"
substraction_type: "normal" # normal or null_it_out